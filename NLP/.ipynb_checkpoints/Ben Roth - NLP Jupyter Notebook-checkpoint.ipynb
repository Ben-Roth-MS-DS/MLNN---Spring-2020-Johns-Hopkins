{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 13\n",
    "Find your favorite news source and grab the article text.\n",
    "\n",
    "1. Show the most common words in the article.\n",
    "2. Show the most common words under a part of speech. (i.e. NOUN: {'Bob':12, 'Alice':4,})\n",
    "3. Find a subject/object relationship through the dependency parser in any sentence.\n",
    "4. Show the most common Entities and their types. \n",
    "5. Find Entites and their dependency (hint: entity.root.head)\n",
    "6. Find the most similar words in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/Broth/anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/Broth/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /Users/Broth/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#download model\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For my assignment, I will using an article from Crooked Media entitled \"Democrats Can Change History or Doom Us To Repeat It\" found (here)[https://crooked.com/articles/democrats-trump-plague/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿In 2018, Republicans lost a statewide vote for the Wisconsin Assembly to Democrats by a wide margin, but won the overwhelming majority of its seats anyhow, so successful were their earlier efforts to\n"
     ]
    }
   ],
   "source": [
    "#read in text file\n",
    "with open('../data/democrats_can_change_history_or_doom_us_to_repeat_it.txt', 'r') as file:\n",
    "    text = file.read().replace('\\n', '').replace('\\\\', '').replace(\"{\\rtf1\\ansi\\ansicpg1252\\cocoartf1561\\cocoasubrtf600{\\fonttbl\\f0\\froman\\fcharset0 Times-Roman;}{\\colortbl;\\red255\\green255\\blue255;\\red20\\green20\\blue20;\\red255\\green255\\blue255;}{\\*\\expandedcolortbl;;\\cssrgb\\c10196\\c10196\\c10196;\\cssrgb\\c100000\\c100000\\c100000;}\\margl1440\\margr1440\\vieww10800\\viewh8400\\viewkind0\\deftab720\\pard\\pardeftab720\\sl560\\sa400\\partightenfactor0\\f0\\fs36 \\cf2 \\cb3 \\expnd0\\expndtw0\\kerning0\\outl0\\strokewidth0 \\strokec2\",'')\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "﻿In 2018, Republicans lost a statewide vote for the Wisconsin Assembly to Democrats by a wide margin, but won the overwhelming majority of its seats anyhow, so successful were their earlier efforts to gerrymander themselves free from accountability to voters. This week, without consent of the governed, they used the power of their ill-gotten majority to assure that another statewide election—this one for a supreme court seat—transpired amid plague conditions. They have, in fact, declined to assist Gov. Tony Evers (D-WI) in any facet of the state’s coronavirus response. That includes both refusing to delay the election, and refusing to allow more people to vote in absentia, let alone doing both. With the citizenry under stay-at-home orders, the number of polling locations in the Democratic stronghold of Milwaukee dwindled from 180 to five.When Evers issued a last-minute emergency order to delay the election, the right-wing majority on the state’s supreme court—a majority directly invested in suppressing voter turnout—voided the"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = spacy.load('en')\n",
    "\n",
    "processed_text = processor(text)\n",
    "processed_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Show the most common words in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common words and corresponding counts are:\n",
      "[('Republicans', 16), ('Trump', 16), ('n’t', 15), ('Democrats', 14), ('’s', 14)]\n"
     ]
    }
   ],
   "source": [
    "tokens = [word.text for word in processed_text if word.is_stop != True and word.is_punct != True]\n",
    "\n",
    "        \n",
    "# five most common tokens\n",
    "word_count = Counter(tokens)\n",
    "most_common = word_count.most_common(5)\n",
    "\n",
    "print('The most common words and corresponding counts are:')\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Show the most common words under a part of speech. (i.e. NOUN: {'Bob':12, 'Alice':4,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [word.pos_ for word in processed_text if word.is_stop != True and word.is_punct != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most common words per part of speech are:\n",
      "            Word PartOfSpeech  Count\n",
      "310          hoc            X      1\n",
      "618         vote         VERB      8\n",
      "102        allow         VERB      4\n",
      "0                       SPACE      1\n",
      "516       second        PUNCT      1\n",
      "71         Trump        PROPN     16\n",
      "55   Republicans        PROPN     14\n",
      "68          They         PRON      9\n",
      "36            It         PRON      4\n",
      "643           ’s         PART     10\n",
      "1            180          NUM      1\n",
      "2             20          NUM      1\n",
      "246     election         NOUN     13\n",
      "195  coronavirus         NOUN      7\n",
      "66           The          DET      8\n",
      "65          That          DET      3\n",
      "14           But        CCONJ      5\n",
      "11           And        CCONJ      3\n",
      "409          n’t          ADV     15\n",
      "375       matter          ADV      3\n",
      "34            If          ADP      4\n",
      "64          That          ADP      2\n",
      "22    Democratic          ADJ      9\n",
      "53    Republican          ADJ      3\n"
     ]
    }
   ],
   "source": [
    "#create dataframe of words and parts of speech\n",
    "df = pd.DataFrame({'Word': tokens,\n",
    "     'PartOfSpeech': parts\n",
    "    })\n",
    "\n",
    "#get count of each word per p.o.s.\n",
    "dfgrp = df.groupby(['Word', 'PartOfSpeech']).size().reset_index(name='Count')\n",
    "\n",
    "#get most common words under a part of speech\n",
    "print(\"The two most common words per part of speech are:\")\n",
    "print(dfgrp.sort_values(['PartOfSpeech','Count'], ascending = False).groupby('PartOfSpeech').head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find a subject/object relationship through the dependency parser in any sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "﻿In 2018, Republicans lost a statewide vote for the Wisconsin Assembly to Democrats by a wide margin, but won the overwhelming majority of its seats anyhow, so successful were their earlier efforts to gerrymander themselves free from accountability to voters."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sentence for sentence in processed_text.sents]\n",
    "\n",
    "first_sentence = sentences[0]\n",
    "\n",
    "first_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the first sentence of the article:\n",
      "   - The word Republicans is a nsubj\n",
      "   - The word vote is a dobj\n",
      "   - The word Assembly is a pobj\n",
      "   - The word Democrats is a pobj\n",
      "   - The word margin is a pobj\n",
      "   - The word majority is a dobj\n",
      "   - The word seats is a pobj\n",
      "   - The word themselves is a dobj\n",
      "   - The word accountability is a pobj\n",
      "   - The word voters is a pobj\n"
     ]
    }
   ],
   "source": [
    "dependencies = {}\n",
    "\n",
    "for word in first_sentence:\n",
    "    #subject would be\n",
    "    if \"subj\" in word.dep_:\n",
    "        dependencies[word] = word.dep_\n",
    "    #iobj for indirect object\n",
    "    elif \"obj\" in word.dep_:\n",
    "        dependencies[word] = word.dep_\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print('In the first sentence of the article:')\n",
    "for key, value in dependencies.items():\n",
    "    print('   - The word', key, 'is a', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Show the most common Entities and their types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most common entities and their types are:\n",
      "           Word    Type  Count\n",
      "22  Republicans    NORP    256\n",
      "13    Democrats    NORP    196\n",
      "25        Trump    NORP     96\n",
      "29    Wisconsin     GPE     90\n",
      "12   Democratic    NORP     90\n",
      "26        Trump     ORG     48\n",
      "27        Trump  PERSON     32\n",
      "10     Congress     ORG     25\n",
      "16          GOP     ORG      9\n",
      "24       Senate     ORG      9\n"
     ]
    }
   ],
   "source": [
    "entities = [entity for entity in processed_text.ents]\n",
    "labels = [entity.label_ for entity in processed_text.ents]\n",
    "\n",
    "df1 = pd.DataFrame({'Word': tokens})\n",
    "\n",
    "df2 = pd.DataFrame({'Entity': entities,\n",
    "     'Type': labels\n",
    "    })\n",
    "\n",
    "df2['Entity'] = df2['Entity'].astype(str).str.replace(')', '').str.replace(\"(\", \"\")\n",
    "\n",
    "df = pd.merge(df1, df2, left_on = 'Word', right_on = 'Entity', how = 'left')\n",
    "\n",
    "dfgrp = df.groupby(['Word', 'Type']).size().reset_index(name='Count')\n",
    "\n",
    "print('The ten most common entities and their types are:')\n",
    "print(dfgrp.sort_values('Count', ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find Entites and their dependency (hint: entity.root.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Entity Dependency\n",
      "0                      (2018)        ﻿In\n",
      "1               (Republicans)       lost\n",
      "2  (the, Wisconsin, Assembly)        for\n",
      "3                 (Democrats)         to\n",
      "4                (This, week)       used\n"
     ]
    }
   ],
   "source": [
    "ent_dep = [entity.root.head.text for entity in entities]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Entity':entities,\n",
    "    'Dependency':ent_dep\n",
    "})\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find the most similar words in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried using the spaCy similarity function, but I kept getting the following error:\n",
    "\n",
    "    ValueError: [E010] Word vectors set to length 0. This may be because you don't have a model installed or loaded, or because your model doesn't include word vectors. For more info, see the docs:\n",
    "    \n",
    " Because of this, I used sklearn's cosine similarity function to find similarities. Unfortuntaley, this can only find similarities for exact words, not necessarily in meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[token.orth_ for token in sentence] for sentence in processed_text.sents]\n",
    "\n",
    "cos_sims = pd.DataFrame(columns = ['Sentence1', 'Sentence2', 'Similarity'])\n",
    "i = 0\n",
    "\n",
    "for x in range(len(sentences)):\n",
    "    for y in range(len(sentences)):\n",
    "        \n",
    "        sent1_list = sentences[x]\n",
    "        sent1_count = dict(Counter(sent1_list))\n",
    "        sent1_count['Sentence'] = 'sent1'\n",
    "\n",
    "        sent2_list = sentences[y]\n",
    "        #count sentences and create id key\n",
    "        sent2_count = dict(Counter(sent2_list))\n",
    "        sent2_count['Sentence'] = 'sent2'\n",
    "\n",
    "        #create dfs, and get vectorized sentence values\n",
    "        df1 = pd.DataFrame(sent1_count, index = ['sent1'])\n",
    "        df2 = pd.DataFrame(sent2_count, index = ['sent2'])\n",
    "\n",
    "        df = pd.concat([df1, df2], axis=0, ignore_index=True, sort = False)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        vals1 = list(df[df['Sentence'] == 'sent1'].drop(columns = ['Sentence'], axis = 1).values)\n",
    "        vals2 = list(df[df['Sentence'] == 'sent2'].drop(columns = ['Sentence'], axis = 1).values)\n",
    "\n",
    "\n",
    "        #find similarities and add to output df\n",
    "        vals1 = list(df[df['Sentence'] == 'sent1'].drop(columns = ['Sentence'], axis = 1).values)\n",
    "        vals2 = list(df[df['Sentence'] == 'sent2'].drop(columns = ['Sentence'], axis = 1).values)\n",
    "\n",
    "        similarity = cosine_similarity(vals1, vals2)\n",
    "        \n",
    "        cos_sims.loc[i] = [x, y, round(similarity[0][0], 3)]\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most similar sentences are: \n",
      "\" First , protect the election from the pandemic , and then win it by a wide - enough margin to pick up the Senate ; second , create the tools they ’ll need now to prevent Republicans from shutting down the rescue unilaterally next year . \",\n",
      "and:\n",
      "\" Against the backdrop of the plague election in Wisconsin , and Republican efforts to hobble the response there , they should be obvious , and easy to make . \"\n"
     ]
    }
   ],
   "source": [
    "cos_filt = cos_sims[cos_sims['Similarity'] < 1.0].sort_values(['Similarity'], ascending = False)\n",
    "\n",
    "cos_head = cos_filt.head(1)\n",
    "\n",
    "fin_sen1 = int(cos_head['Sentence1'].values[0])\n",
    "fin_sen2 = int(cos_head['Sentence2'].values[0])\n",
    "\n",
    "print('The two most similar sentences are: ')\n",
    "print('\"', ' '.join(sentences[fin_sen1]), '\",')\n",
    "print('and:')\n",
    "print('\"', \" \".join(sentences[fin_sen2]),'\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
